\chapter{Estado de la Cuestión}
\label{cap:estadoDeLaCuestion}

En este apartado expondremos el estado actual de los puntos principales de nuestro proyecto según las investigaciones realizadas, así como los trabajos o artículos relacionados con los temas a tratar. Estos son, entre otros, trabajos relacionados con los principales proveedores Cloud y su comparación, trabajos que traten con grandes volúmenes de datos públicos, o trabajos que utilicen diferentes \ac{IA}s para el tratamiento de datos y la obtención de conclusiones a partir de estos. 

Aunque también usaremos datos, estudios y aplicaciones de otras partes del globo, nos intentaremos centrar en datos del territorio español, ya que la cantidad de datos de toda la web es inmensa y de esta manera acotaremos el alcance del proyecto y contribuiremos a aprovechar datos que no han sido tan explotados y explorados como pueden ser los datos abiertos de Google \citep{googleDataCommon} o Amazon \citep{AWSDataCommon} . 

\section{Datos}

Llevamos mucho tiempo escuchando que vivimos en la era de la información o de los dato, desde la invención del transistor en 1947 \citep{wikiInformationAge}, pasando la primera vez que se acuño el termino 'Big data' y 'web 2.0' en 2005 \citep{HistoryOfBigData}, así como su rápido crecimiento y adopción en todas las esferas \citep{brown2011you}, hasta el presente, donde los datos y su tratamiento a través de múltiples herramientas, incluyendo la recientemente omnipresente Inteligencia Artificial, llegaran a generan ,según proyecciones, la asombrosa cifra de 149 Zettabytes de datos en 2024, ¡¡23 ceros en bytes!! \citep{BigDataStatista} \& \citep{BigDataStadisticsMarket}.

En España, los datos también muestran un aumento significativo, según los datos de telecomunicaciones del CNMC, los cuales se han analizado con este mismo proyecto \citep{DatosGeneralesCNMC}, el uso de datos generales en las principales empresas es de 0.092 Zettabytes de datos en 2024.
Esto es solo un 0.06\% del volumen global, lo cual no cuadra del todo con otras estimaciones \citep{DatosMercadosEspanna}, que por volumen de mercado sitúan a España en un 0.9\% del volumen global, lo cual se puede explicar debido a que el CNMC solo toma en cuenta datos de las principales empresas de telecomunicaciones. 

Aun con estas discrepancias en cuanto a números, lo que esta claro es que el mercado de los datos no para de crecer año tras año y cada vez resulta mas difícil separar la información relevante del ruido, evitando la 'infoxicación' o sobrecarga informativa \citep{Infoxicacion}. En este escenario, tecnologías como la computación en la nube e inteligencia artificial pueden ser claves para encontrar los patrones o llegar a conclusiones.

Mencionar también brevemente que los 'datos' no suelen aparecer en formatos consistentes, y para este trabajo se han tratado diferentes formatos: CSV, JSON, bases de datos diversas, excel, APIs, etc. \citep{khan2019fileFormats}. Esto es así porque queríamos que las fuentes de datos fueran heterogéneas y no excluir datos por que su extracción o tratamiento fueran complejos, ya que este es el caso para la mayoría de aplicaciones en el mundo real. Esto se explicara mas en detalle en el apartado de Materiales y métodos \ref{cap:Materiales y metodos}.

Lo primero para la realización de este proyecto, es la obtención de datos públicos. Esto presenta tres grandes complicaciones a tener en cuenta:

La primera es que \textbf{no todos los datos que deberían ser públicos lo son}, y cuando lo son, su acceso y tratamiento es complicado, ya sea a conciencia o por indolencia. Según la OECD \citep{OECD2023openData}, sólo el 48\% de los conjuntos de datos de gran valor están disponibles como datos abiertos en los países estudiados, datos que bajan al 30\% cuando se trata de datos financieros. y estudios de otras partes del mundo también avalan esta reticencia a la correcta apertura de información publica \citep{TransparenciaEcuador}, \citep{TransparenciaMexico}.


La segunda causa es \textbf{la regulación}, el tratamiento de datos en Europa debe seguir la RGPD de 2016 y las regulaciones propias de cada estado \citep{LicenciasLibres2017Datos} \& \citep{webRGPD2016Europa}, así como la mas reciente Ley de Gobernanza de Datos \citep{webLGD2023Europa} \& \citep{DatosAbiertos2022Cloud}. Para cumplir con estas normativas, en este proyecto nos centraremos en el uso de datos oficiales abiertos, evitando técnicas como el 'scraping' que pueden estar sujetas a controversia a la vista de estas regulaciones. También se verificaran las licencias de todos los datos y modelos utilizados para asegurarnos de que no incumplimos ninguna de las regulaciones existentes.

En cuanto a datos de otros países fuera de la Unión Europea, tenemos panoramas diversos los cuales vale la pena mencionar, desde una regulación mas laxa en Estados Unidos, hasta un control estricto en países como China. Estos datos no se utilizaran en este trabajo por temas de alcance, ya que se prefiere dar prioridad a fuentes de datos nacionales, pero las herramientas desarrolladas serian aplicables a estos mismos datos cumpliendo sus normativas.

En Estados Unidos, el panorama es sobretodo abierto, pero fragmentado. Cuentan con regulaciones sectoriales, como la 'Health Insurance Portability and Accountability Act' (HIPAA) para datos médicos \citep{webHIPAA1996EEUU} y regulaciones estatales como la 'California Consumer Privacy Act' (CCPA) \citep{webCCPA2018California} para proteger derechos individuales. También existe una legislación nacional que promueve los datos abiertos, la 'OPEN Government Data Act' (2019) \citep{webOGDA2019EEUU}, que establece que los datos gubernamentales deben ser abiertos y utilizables.

Por su parte, China ha establecido un marco regulatorio estricto con leyes como la 'Personal Information Protection Law' (PIPL) \citep{webPIPL2021China}, que habla de principios de consentimiento y derechos del individuo, y la 'Data Security Law' (DSL) \citep{webDSL2021China}, que prioriza la seguridad nacional y el control sobre los datos generados en el país.


Por ultimo, la tercera causa es \textbf{la tecnología}, como ya hemos hablado, los datos pueden estar en formatos diferentes, y la cantidad de herramientas para su tratamiento va en aumento, y hay que tener en cuenta también la integración, el procesamiento escalable a la cantidad de datos en aumento y el gobierno de los flujos de datos y modelos en un entorno 'cloud' que está en evolución constante. Por ello, en este trabajo se ha optado por emplear herramientas ampliamente extendidas, soportadas, y principalmente abiertas, así como intentar hacer del conjunto de ellas lo mas amplio posible, para estudiar y comparar un amplio abanico de soluciones.


	\subsection{Trabajos anteriores}
	
	A parte de todas las referencias ya incluidas en esta sección, me gustaría destacar todo el trabajo de Jaime Gómez-Obregón para liberar y hacer accesibles los datos de España \citep{JaimeGomezObregon}, con acciones como publicar las subvenciones a las empresas en España a través del portal ministerial y hacerlas accesibles  \citep{JaimeGomezObregonSubvenciones}, o estudios sobre donaciones sospechosas de corrupción \citep{JaimeGomezObregonDonacion}. Todo este trabajo ha guiado también a este proyecto hacia un uso ético de los datos.
	
	
	\subsection{Conjuntos de datos}
	
	A nivel institucional, Europa tiene su propio portal para acceder a datos públicos \citep{PortalDatosEuropa}, y a nivel nacional, el Instituto Nacional de Estadística (INE) y la Agencia Tributaria han sido actores clave en la liberación de datos abiertos y el fomento de su reutilización para la investigación e innovación, impulsando proyectos como el Portal de Transparencia del Gobierno de España y las iniciativas de datos abiertos de comunidades autónomas y ayuntamientos \citep{PortalDatosGob}; \citep{PortalDatosMadrid} \& \citep{PortalRegistradores}, los cual se esfuerzan por hacer públicos datos de alto valor [\ref{Appendix:1}]. También destacar iniciativas que fomentan su transparencia, como InfoParticipa \citep{PortalInfoparticipa} o iniciativas privadas para la recolección de datos públicos \citep{PortalEsriEspanna}. Por ultimo, también añadir a la interminable lista de datos públicos disponibles iniciativas individuales como "Awesome public datasets" \citep{awesomePublicDatasets} que se dedica a recopilar fuentes de datos fiables (aunque en este caso principalmente de Estados Unidos), o iniciativas como UniversiDATA \citep{UniversiDATA}.
	
	Todos estos portales y aplicaciones son de gran importancia y constituyen la base material sobre la que se sustentan trabajos como el presente.
	
\section{Nubes}


\subsection{Principales Proveedores de Nube y sus Capas Gratuitas}
\label{sec:cloud-free-tiers}

A continuación detallaremos las pruebas gratuitas de los principales proveedores de servicios en la nube, información crucial para la selección tecnológica de este proyecto. Solo se listarán sus principales servicios, ya que la lista total es muy extensa \citep{free-for-dev}.

\subsubsection*{Google Cloud Platform}
\begin{itemize}
	\item \textbf{App Engine}: 28 horas/día de ejecución 'frontend', 9 horas/día de ejecución 'backend'.
	\item \textbf{Cloud Firestore}: 1GB almacenamiento, 50.000 lecturas, 20.000 escrituras, 20.000 borrados por día.
	\item \textbf{Compute Engine}: 1 e2-micro no susceptible de interrupción, 30GB disco duro, 5GB de instantáneas, con regiones restringidas.
	\item \textbf{Cloud Storage}: 5GB, 1GB de tráfico de salida de red.
	\item \textbf{Cloud Shell}: Terminal Linux basado en web con 5GB de almacenamiento persistente. Límite de 60 horas/semana.
	\item \textbf{Cloud Pub/Sub}: 10GB de mensajes por mes.
	\item \textbf{Cloud Functions}: 2 millones de invocaciones por mes.
	\item \textbf{Cloud Run}: 2M de peticiones por mes, 360.000 GB/segundos de memoria, 180.000 segundos de CPU virtual.
	\item \textbf{Google Kubernetes Engine}: Sin tarifa de gestión de clústeres para un clúster zonal.
	\item \textbf{BigQuery}: 1 TB de consultas por mes, 10 GB de almacenamiento.
	\item \textbf{Cloud Build}: 120 minutos de construcción por día.
	\item \textbf{Cloud Source Repositories}: Hasta 5 usuarios, 50 GB de almacenamiento, 50 GB de tráfico de salida.
	\item \textbf{Google Colab}: Entorno gratuito de desarrollo con 'Jupyter Notebooks'.
	\item \textbf{Lista completa}: \url{https://cloud.google.com/free}
\end{itemize}

\subsubsection*{Amazon Web Services}
\begin{itemize}
	\item \textbf{CloudFront}: 1TB de tráfico de salida por mes y 2M invocaciones de funciones.
	\item \textbf{CloudWatch}: 10 métricas personalizadas y 10 alarmas.
	\item \textbf{CodeBuild}: 100min de tiempo de ejecución por mes.
	\item \textbf{CodeCommit}: 5 usuarios activos, 50GB almacenamiento, 10000 peticiones por mes.
	\item \textbf{CodePipeline}: 1 pipeline activo por mes.
	\item \textbf{DynamoDB}: 25GB base de datos NoSQL.
	\item \textbf{EC2}: 750 horas/mes de t2.micro o t3.micro, 12 meses.
	\item \textbf{EBS}: 30GB por mes de SSD propósito general o magnético, 12 meses.
	\item \textbf{Elastic Load Balancing}: 750 horas por mes, 12 meses.
	\item \textbf{RDS}: 750 horas/mes de db.t2.micro, 20GB almacenamiento SSD, 12 meses.
	\item \textbf{S3}: 5GB almacenamiento estándar, 20K peticiones Get, 2K peticiones Put, 12 meses.
	\item \textbf{Glacier}: 10GB almacenamiento a largo plazo.
	\item \textbf{Lambda}: 1 millón de peticiones por mes.
	\item \textbf{SNS}: 1 millón de publicaciones por mes.
	\item \textbf{SES}: 3.000 mensajes por mes, 12 meses.
	\item \textbf{SQS}: 1 millón de peticiones de colas de mensajería.
	\item \textbf{Lista completa}: \url{https://aws.amazon.com/free/}
\end{itemize}

\subsubsection*{Microsoft Azure}
\begin{itemize}
	\item \textbf{Virtual Machines}: 1 B1S Linux VM, 1 B1S Windows VM, 12 meses.
	\item \textbf{App Service}: 10 aplicaciones web, móviles o de API, con 60 minutos CPU/día.
	\item \textbf{Functions}: 1 millón de peticiones por mes.
	\item \textbf{DevTest Labs}: Entornos de desarrollo y pruebas.
	\item \textbf{Active Directory}: 500.000 objetos.
	\item \textbf{Azure DevOps}: 5 usuarios activos, repositorios Git privados ilimitados.
	\item \textbf{Azure Pipelines}: 10 trabajos paralelos con minutos ilimitados para código abierto.
	\item \textbf{Microsoft IoT Hub}: 8.000 mensajes por día.
	\item \textbf{Load Balancer}: 1 IP pública con balanceo de carga gratuita.
	\item \textbf{Notification Hubs}: 1 millón de notificaciones 'push'.
	\item \textbf{Ancho de banda}: 15GB de entrada y 5GB de salida por mes, 12 meses.
	\item \textbf{Cosmos DB}: 25GB almacenamiento y 1000 unidades de solicitud de rendimiento
	\item \textbf{Static Web Apps}: Aplicaciones estáticas con SSL, autenticación y dominios personalizados
	\item \textbf{Storage}: 5GB almacenamiento de archivos o 'blobs' con redundancia local, 12 meses.
	\item \textbf{Cognitive Services}: APIs de IA/ML con transacciones limitadas.
	\item \textbf{Cognitive Search}: Búsqueda basada en IA, para 10.000 documentos.
	\item \textbf{Azure Kubernetes Service}: Servicio Kubernetes gestionado, gestión de clústeres.
	\item \textbf{Event Grid}: 100K operaciones/mes.
	\item \textbf{Lista completa}: \url{https://azure.microsoft.com/free/}
\end{itemize}

\subsubsection*{Oracle Cloud}
\begin{itemize}
	\item \textbf{Compute}: 2 máquinas virtuales AMD con 1/8 OCPU y 1 GB memoria cada una.
	\item \textbf{Block Volume}: 2 volúmenes, 200 GB total para computación.
	\item \textbf{Object Storage}: 10 GB.
	\item \textbf{Load Balancer}: 1 instancia con 10 Mbps.
	\item \textbf{Databases}: 2 bases de datos, 20 GB cada una.
	\item \textbf{Monitoring}: 500 millones de puntos de ingesta de datos, 1 millardo de recuperación.
	\item \textbf{Ancho de banda}: 10 TB de tráfico de salida por mes, velocidad limitada a 50 Mbps.
	\item \textbf{IP Pública}: 2 IPv4 para máquinas virtuales, 1 IPv4 para balanceador de carga.
	\item \textbf{Notifications}: 1 millón de opciones de entrega por mes, 1000 emails enviados por mes.
	\item \textbf{Lista completa}: \url{https://www.oracle.com/cloud/free/}
\end{itemize}

\subsubsection*{IBM Cloud}
\begin{itemize}
	\item \textbf{Cloudant database}: 1 GB de almacenamiento de datos.
	\item \textbf{Db2 database}: 100MB de almacenamiento de datos.
	\item \textbf{API Connect}: 50.000 llamadas API por mes.
	\item \textbf{Availability Monitoring}: 3 millones de puntos de datos por mes.
	\item \textbf{Log Analysis}: 500MB de registros diarios.
	\item \textbf{Lista completa}: \url{https://www.ibm.com/cloud/free/}
\end{itemize}

\subsubsection*{Cloudflare}
\begin{itemize}
	\item \textbf{Application Services}: DNS, Protección DDoS, CDN con SSL, Firewall de aplicaciones web.
	\item \textbf{Zero Trust \& SASE}: Hasta 50 usuarios, 24 horas de registro de actividad.
	\item \textbf{Cloudflare Tunnel}: Exponer puertos HTTP locales a través de túnel.
	\item \textbf{Workers}: Desplegar código sin servidor - 100k peticiones diarias.
	\item \textbf{Workers KV}: 100k lecturas diarias, 1000 escrituras diarias, 1 GB datos almacenados.
	\item \textbf{R2}: 10 GB por mes, 1 millón operaciones por mes.
	\item \textbf{D1}: 5 millones de filas leídas por día, 100k filas escritas por día, 1 GB almacenamiento.
	\item \textbf{Pages}: Desplegar aplicaciones web - 500 despliegues mensuales, 100 dominios personalizados.
	\item \textbf{Queues}: 1 millón de operaciones por mes.
	\item \textbf{TURN}: 1TB de tráfico saliente por mes.
	\item \textbf{Lista completa}: \url{https://www.cloudflare.com/plans/free/}
\end{itemize}

También, aunque no son nubes propiamente dichas, hemos querido añadir en esta sección otras herramientas que tienen interés para el proyecto:

\subsubsection*{Hugging Face Spaces}
\begin{itemize}
	\item \textbf{Tipo}: Plataforma para desplegar, compartir y descubrir modelos de Aprendizaje Automático (MLOps). Esencial para proyectos de IA. Permite desplegar demostraciones de modelos con interfaz web de forma sencilla.
	\item \textbf{Capa Gratuita - CPU}: 
	\begin{itemize}
		\item 2 CPUs virtuales por espacio.
		\item 16 GB de RAM.
		\item Espacio de almacenamiento: 50 GB (para modelos, datos y código).
		\item Ancho de banda: 100 MB/hora para CPUs.
		\item \textbf{Apagado automático}: Los espacios se suspenden tras 48 horas de inactividad para ahorrar recursos, reactivándose con la siguiente visita.
	\end{itemize}
	\item \textbf{Capa Gratuita - GPU (T4)}: 
	\begin{itemize}
		\item Acceso a una GPU NVIDIA T4 por espacio.
		\item 16 GB de RAM.
		\item Espacio de almacenamiento: 50 GB.
		\item Ancho de banda: 30 MB/hora para GPUs.
		\item Uso: Hasta 30 horas de uso de GPU por mes, pero sujeto a disponibilidad.
		\item \textbf{Apagado automático}: Las GPU se apagan automáticamente tras 1 hora de inactividad.
	\end{itemize}
	\item \textbf{Enfoque}: Despliegue, demostración y compartición de modelos de IA. Integración nativa con el Hub de modelos y conjuntos de datos.
	\item \textbf{URL}: \url{https://huggingface.co/spaces}
\end{itemize}

\subsubsection*{Kaggle Kernels/Notebooks}
\begin{itemize}
	\item \textbf{Tipo}: Entorno de ejecución para cuadernos 'Jupyter' en la nube. Proporciona acceso gratuito a aceleradores hardware de gama alta, eliminando la barrera de entrada para entrenar modelos complejos.
	\item \textbf{Capa Gratuita - Sesiones de Ejecución}:
	\begin{itemize}
		\item \textbf{GPU (NVIDIA Tesla P100)}: Hasta 30 horas por semana (4.3h/día aprox.).
		\item \textbf{TPU (v3)}: Hasta 20 horas por semana (2.8h/día aprox.).
		\item \textbf{CPU}: 20 horas de tiempo total por semana, sin límite de sesiones concurrentes.
	\end{itemize}
	\item \textbf{Límites por Sesión}:
	\begin{itemize}
		\item \textbf{Tiempo máximo de ejecución}: 12 horas por sesión. Tras este tiempo, el kernel se detiene automáticamente.
		\item \textbf{Internet}: Los cuadernos deben tener la opción de Internet activada manualmente para acceder a datos externos o instalar librerías.
		\item \textbf{Almacenamiento Volátil}: 20 GB de espacio temporal de disco. Los datos no persisten entre sesiones, aunque se puede usar el sistema de conjuntos de datos de Kaggle para almacenamiento persistente.
	\end{itemize}
	\item \textbf{Enfoque}: Análisis exploratorio de datos, competiciones de ML y, crucialmente, \textbf{entrenamiento de modelos} que requieran GPU/TPU.
	\item \textbf{URL}: \url{https://www.kaggle.com/code}
\end{itemize}


	\subsection{Trabajos anteriores}


\section{Inteligencia Artificial}

[TODO]

Tener en cuenta también la nueva normativa que la Unión europea ha establecido con el Reglamento de Inteligencia Artificial \citep{webRIA2024Europa}, el cual se ha tenido de base para el uso de IA en este proyecto, intentando aplicar buenas practicas al uso de las mismas, así como documentar las fuentes de datos, métodos de anonimización y posibles sesgos.
	
	\subsection{Trabajos anteriores}


