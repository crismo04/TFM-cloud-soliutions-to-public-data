\chapter{Estado de la Cuestión}
\label{cap:estadoDeLaCuestion}

En este apartado expondremos el estado actual de los puntos principales de nuestro proyecto según las investigaciones realizadas, así como los trabajos o artículos relacionados con los temas a tratar. Estos son, entre otros, trabajos relacionados con los principales proveedores Cloud y su comparación, trabajos que traten con grandes volúmenes de datos públicos, o trabajos que utilicen diferentes IAs para el tratamiento de datos y la obtención de conclusiones a partir de estos. 

Aunque también usaremos datos, estudios y aplicaciones de otras partes del globo, nos intentaremos centrar en datos del territorio español, ya que la cantidad de datos de toda la web es inmensa y de esta manera acotaremos el alcance del proyecto y contribuiremos a aprovechar datos que no han sido tan explotados y explorados como pueden ser los datos abiertos de Google \citep{googleDataCommon} o Amazon \citep{AWSDataCommon} . 

\section{Datos}

Llevamos mucho tiempo escuchando que vivimos en la era de la información o de los dato, desde la invención del transistor en 1947 \citep{wikiInformationAge}, pasando la primera vez que se acuño el termino 'Big data' y 'web 2.0' en 2005 \citep{HistoryOfBigData}, así como su rápido crecimiento y adopción en todas las esferas \citep{brown2011you}, hasta el presente, donde los datos y su tratamiento a través de múltiples herramientas, incluyendo la recientemente omnipresente Inteligencia Artificial, llegaran a generan ,según proyecciones, la asombrosa cifra de 149 Zettabytes de datos en 2024, ¡¡23 ceros en bytes!! \citep{BigDataStatista} \& \citep{BigDataStadisticsMarket}.

En España, los datos también muestran un aumento significativo, según los datos de telecomunicaciones del CNMC, los cuales se han analizado con este mismo proyecto \citep{DatosGeneralesCNMC}, el uso de datos generales en las principales empresas es de 0.092 Zettabytes de datos en 2024.
Esto es solo un 0.06\% del volumen global, lo cual no cuadra del todo con otras estimaciones \citep{DatosMercadosEspanna}, que por volumen de mercado sitúan a España en un 0.9\% del volumen global, lo cual se puede explicar debido a que el CNMC solo toma en cuenta datos de las principales empresas de telecomunicaciones. 

Aun con estas discrepancias en cuanto a números, lo que esta claro es que el mercado de los datos no para de crecer año tras año y cada vez resulta mas difícil separar la información relevante del ruido, evitando la 'infoxicación' o sobrecarga informativa \citep{Infoxicacion}. En este escenario, tecnologías como la computación en la nube e inteligencia artificial pueden ser claves para encontrar los patrones o llegar a conclusiones.

Mencionar también brevemente que los 'datos' no suelen aparecer en formatos consistentes, y para este trabajo se han tratado diferentes formatos: CSV, JSON, bases de datos diversas, excel, APIs, etc. \citep{khan2019fileFormats}. Esto es así porque queríamos que las fuentes de datos fueran heterogéneas y no excluir datos por que su extracción o tratamiento fueran complejos, ya que este es el caso para la mayoría de aplicaciones en el mundo real. Esto se explicara mas en detalle en el apartado de Materiales y métodos \ref{cap:Materiales y metodos}.

Lo primero para la realización de este proyecto, es la obtención de datos públicos. Esto presenta tres grandes complicaciones a tener en cuenta:

La primera es que \textbf{no todos los datos que deberían ser públicos lo son}, y cuando lo son, su acceso y tratamiento es complicado, ya sea a conciencia o por indolencia. Según la OECD \citep{OECD2023openData}, sólo el 48\% de los conjuntos de datos de gran valor están disponibles como datos abiertos en los países estudiados, datos que bajan al 30\% cuando se trata de datos financieros. y estudios de otras partes del mundo también avalan esta reticencia a la correcta apertura de información publica \citep{TransparenciaEcuador}, \citep{TransparenciaMexico}.


La segunda causa es \textbf{la regulación}, el tratamiento de datos en Europa debe seguir la RGPD de 2016 y las regulaciones propias de cada estado \citep{LicenciasLibres2017Datos} \& \citep{webRGPD2016Europa}, así como la mas reciente Ley de Gobernanza de Datos \citep{webLGD2023Europa} \& \citep{DatosAbiertos2022Cloud}. Para cumplir con estas normativas, en este proyecto nos centraremos en el uso de datos oficiales abiertos, evitando técnicas como el 'scraping' que pueden estar sujetas a controversia a la vista de estas regulaciones. También se verificaran las licencias de todos los datos y modelos utilizados para asegurarnos de que no incumplimos ninguna de las regulaciones existentes.

En cuanto a datos de otros países fuera de la Unión Europea, tenemos panoramas diversos los cuales vale la pena mencionar, desde una regulación mas laxa en Estados Unidos, hasta un control estricto en países como China. Estos datos no se utilizaran en este trabajo por temas de alcance, ya que se prefiere dar prioridad a fuentes de datos nacionales, pero las herramientas desarrolladas serian aplicables a estos mismos datos cumpliendo sus normativas.

En Estados Unidos, el panorama es sobretodo abierto, pero fragmentado. Cuentan con regulaciones sectoriales, como la 'Health Insurance Portability and Accountability Act' (HIPAA) para datos médicos \citep{webHIPAA1996EEUU} y regulaciones estatales como la 'California Consumer Privacy Act' (CCPA) \citep{webCCPA2018California} para proteger derechos individuales. También existe una legislación nacional que promueve los datos abiertos, la 'OPEN Government Data Act' (2019) \citep{webOGDA2019EEUU}, que establece que los datos gubernamentales deben ser abiertos y utilizables.

Por su parte, China ha establecido un marco regulatorio estricto con leyes como la 'Personal Information Protection Law' (PIPL) \citep{webPIPL2021China}, que habla de principios de consentimiento y derechos del individuo, y la 'Data Security Law' (DSL) \citep{webDSL2021China}, que prioriza la seguridad nacional y el control sobre los datos generados en el país.


Por ultimo, la tercera causa es \textbf{la tecnología}, como ya hemos hablado, los datos pueden estar en formatos diferentes, y la cantidad de herramientas para su tratamiento va en aumento, y hay que tener en cuenta también la integración, el procesamiento escalable a la cantidad de datos en aumento y el gobierno de los flujos de datos y modelos en un entorno 'cloud' que está en evolución constante. Por ello, en este trabajo se ha optado por emplear herramientas ampliamente extendidas, soportadas, y principalmente abiertas, así como intentar hacer del conjunto de ellas lo mas amplio posible, para estudiar y comparar un amplio abanico de soluciones.


	\subsection{Trabajos anteriores}
	
	A parte de todas las referencias ya incluidas en esta sección, me gustaría destacar todo el trabajo de Jaime Gómez-Obregón para liberar y hacer accesibles los datos de España \citep{JaimeGomezObregon}, con acciones como publicar las subvenciones a las empresas en España a través del portal ministerial y hacerlas accesibles  \citep{JaimeGomezObregonSubvenciones}, o estudios sobre donaciones sospechosas de corrupción \citep{JaimeGomezObregonDonacion}. Todo este trabajo ha guiado también a este proyecto hacia un uso ético de los datos.
	
	
	\subsection{Conjuntos de datos}
	
	A nivel institucional, Europa tiene su propio portal para acceder a datos públicos \citep{PortalDatosEuropa}, y a nivel nacional, el Instituto Nacional de Estadística (INE) y la Agencia Tributaria han sido actores clave en la liberación de datos abiertos y el fomento de su reutilización para la investigación e innovación, impulsando proyectos como el Portal de Transparencia del Gobierno de España y las iniciativas de datos abiertos de comunidades autónomas y ayuntamientos \citep{PortalDatosGob}; \citep{PortalDatosMadrid} \& \citep{PortalRegistradores}, los cual se esfuerzan por hacer públicos datos de alto valor [\ref{Appendix:1}]. También destacar iniciativas que fomentan su transparencia, como InfoParticipa \citep{PortalInfoparticipa} o iniciativas privadas para la recolección de datos públicos \citep{PortalEsriEspanna}. Por ultimo, también añadir a la interminable lista de datos públicos disponibles iniciativas individuales como "Awesome public datasets" \citep{awesomePublicDatasets} que se dedica a recopilar fuentes de datos fiables (aunque en este caso principalmente de Estados Unidos), o iniciativas como UniversiDATA \citep{UniversiDATA}.
	
	Todos estos portales y aplicaciones son de gran importancia y constituyen la base material sobre la que se sustentan trabajos como el presente.
	
\section{Nubes}


\subsection{Principales Proveedores de Cloud y sus Free Tiers}
\label{sec:cloud-free-tiers}

A continuación detallaremos las pruebas gratuitas de los principales proveedores de servicios en la nube, información crucial para la selección tecnológica de este proyecto. Solo se listaran sus principales servicios, ya que la lista total es muy extensa \citep{free-for-dev}.

\subsubsection*{Google Cloud Platform}
\begin{itemize}
	\item \textbf{App Engine}: 28 horas/día de instancias frontend, 9 horas/día de backend
	\item \textbf{Cloud Firestore}: 1GB almacenamiento, 50.000 lecturas, 20.000 escrituras, 20.000 borrados por día
	\item \textbf{Compute Engine}: 1 e2-micro no preemptible, 30GB HDD, 5GB de snapshots (regiones restringidas)
	\item \textbf{Cloud Storage}: 5GB, 1GB de egress de red
	\item \textbf{Cloud Shell}: Shell Linux basado en web con 5GB de almacenamiento persistente. Límite de 60 horas/semana
	\item \textbf{Cloud Pub/Sub}: 10GB de mensajes por mes
	\item \textbf{Cloud Functions}: 2 millones de invocaciones por mes
	\item \textbf{Cloud Run}: 2 millones de requests por mes, 360.000 GB-seconds de memoria, 180.000 vCPU-seconds
	\item \textbf{Google Kubernetes Engine}: Sin fee de gestión de clusters para un cluster zonal
	\item \textbf{BigQuery}: 1 TB de consultas por mes, 10 GB de almacenamiento
	\item \textbf{Cloud Build}: 120 minutos-build por día
	\item \textbf{Cloud Source Repositories}: Hasta 5 usuarios, 50 GB almacenamiento, 50 GB egress
	\item \textbf{Google Colab}: Entorno gratuito de desarrollo con Jupyter Notebooks
	\item \textbf{Lista completa}: \url{https://cloud.google.com/free}
\end{itemize}

\subsubsection*{Amazon Web Services}
\begin{itemize}
	\item \textbf{CloudFront}: 1TB egress por mes y 2M invocaciones de funciones
	\item \textbf{CloudWatch}: 10 métricas personalizadas y 10 alarmas
	\item \textbf{CodeBuild}: 100min de tiempo de build por mes
	\item \textbf{CodeCommit}: 5 usuarios activos, 50GB almacenamiento, 10000 requests por mes
	\item \textbf{CodePipeline}: 1 pipeline activo por mes
	\item \textbf{DynamoDB}: 25GB base de datos NoSQL
	\item \textbf{EC2}: 750 horas/mes de t2.micro o t3.micro (12 meses)
	\item \textbf{EBS}: 30GB por mes de SSD propósito general o magnético (12 meses)
	\item \textbf{Elastic Load Balancing}: 750 horas por mes (12 meses)
	\item \textbf{RDS}: 750 horas/mes de db.t2.micro, 20GB almacenamiento SSD (12 meses)
	\item \textbf{S3}: 5GB almacenamiento estándar, 20K Get requests, 2K Put requests (12 meses)
	\item \textbf{Glacier}: 10GB almacenamiento a largo plazo
	\item \textbf{Lambda}: 1 millón de requests por mes
	\item \textbf{SNS}: 1 millón de publicaciones por mes
	\item \textbf{SES}: 3.000 mensajes por mes (12 meses)
	\item \textbf{SQS}: 1 millón de requests de colas de mensajería
	\item \textbf{Lista completa}: \url{https://aws.amazon.com/free/}
\end{itemize}

\subsubsection*{Microsoft Azure}
\begin{itemize}
	\item \textbf{Virtual Machines}: 1 B1S Linux VM, 1 B1S Windows VM (12 meses)
	\item \textbf{App Service}: 10 web, mobile o API apps (60 minutos CPU/día)
	\item \textbf{Functions}: 1 millón de requests por mes
	\item \textbf{DevTest Labs}: Entornos de desarrollo y testing
	\item \textbf{Active Directory}: 500.000 objetos
	\item \textbf{Active Directory B2C}: 50.000 usuarios almacenados mensuales
	\item \textbf{Azure DevOps}: 5 usuarios activos, repositorios Git privados ilimitados
	\item \textbf{Azure Pipelines}: 10 jobs paralelos gratis con minutos ilimitados para open source
	\item \textbf{Microsoft IoT Hub}: 8.000 mensajes por día
	\item \textbf{Load Balancer}: 1 IP pública load-balanced gratuita (VIP)
	\item \textbf{Notification Hubs}: 1 millón de notificaciones push
	\item \textbf{Bandwidth}: 15GB inbound (12 meses) \& 5GB egress por mes
	\item \textbf{Cosmos DB}: 25GB almacenamiento y 1000 RUs de throughput
	\item \textbf{Static Web Apps}: Apps estáticas con SSL gratis, autenticación y dominios custom
	\item \textbf{Storage}: 5GB LRS File o Blob storage (12 meses)
	\item \textbf{Cognitive Services}: APIs de AI/ML con transacciones limitadas gratis
	\item \textbf{Cognitive Search}: Búsqueda basada en AI, gratis para 10.000 documentos
	\item \textbf{Azure Kubernetes Service}: Servicio Kubernetes gestionado, gestión de clusters gratis
	\item \textbf{Event Grid}: 100K operaciones/mes
	\item \textbf{Lista completa}: \url{https://azure.microsoft.com/free/}
\end{itemize}

\subsubsection*{Oracle Cloud}
\begin{itemize}
	\item \textbf{Compute}: 2 VMs AMD con 1/8 OCPU y 1 GB memoria cada una
	\item \textbf{Block Volume}: 2 volúmenes, 200 GB total (para compute)
	\item \textbf{Object Storage}: 10 GB
	\item \textbf{Load balancer}: 1 instancia con 10 Mbps
	\item \textbf{Databases}: 2 DBs, 20 GB cada una
	\item \textbf{Monitoring}: 500 millones de datos de ingestion, 1 billón de recuperación
	\item \textbf{Bandwidth}: 10 TB egress por mes, velocidad limitada a 50 Mbps
	\item \textbf{Public IP}: 2 IPv4 para VMs, 1 IPv4 para load balancer
	\item \textbf{Notifications}: 1 millón de opciones de entrega por mes, 1000 emails enviados por mes
	\item \textbf{Lista completa}: \url{https://www.oracle.com/cloud/free/}
\end{itemize}

\subsubsection*{IBM Cloud}
\begin{itemize}
	\item \textbf{Cloudant database}: 1 GB de almacenamiento de datos
	\item \textbf{Db2 database}: 100MB de almacenamiento de datos
	\item \textbf{API Connect}: 50.000 llamadas API por mes
	\item \textbf{Availability Monitoring}: 3 millones de datos por mes
	\item \textbf{Log Analysis}: 500MB de logs diarios
	\item \textbf{Lista completa}: \url{https://www.ibm.com/cloud/free/}
\end{itemize}

\subsubsection*{Cloudflare}
\begin{itemize}
	\item \textbf{Application Services}: DNS gratis, DDoS Protection, CDN con SSL gratis, WAF
	\item \textbf{Zero Trust \& SASE}: Hasta 50 usuarios, 24 horas de logging de actividad
	\item \textbf{Cloudflare Tunnel}: Exponer puertos HTTP locales through tunnel
	\item \textbf{Workers}: Desplegar código serverless - 100k requests diarios
	\item \textbf{Workers KV}: 100k lecturas diarias, 1000 escrituras diarias, 1 GB datos almacenados
	\item \textbf{R2}: 10 GB por mes, 1 millón operaciones Class A por mes
	\item \textbf{D1}: 5 millones de filas leídas por día, 100k filas escritas por día, 1 GB almacenamiento
	\item \textbf{Pages}: Desplegar web apps - 500 builds mensuales, 100 dominios custom
	\item \textbf{Queues}: 1 millón de operaciones por mes
	\item \textbf{TURN}: 1TB de tráfico saliente gratis por mes
	\item \textbf{Lista completa}: \url{https://www.cloudflare.com/plans/free/}
\end{itemize}

También, aunque no son nubes propiamente dichas, hemos querido añadir en esta sección otras herramientas que tienen interés para el proyecto:

\subsubsection*{Hugging Face Spaces}
\begin{itemize}
	\item \textbf{Tipo}: Plataforma para desplegar, compartir y descubrir modelos de Machine Learning (MLOps).
	\item \textbf{Relevancia}: Esencial para proyectos de IA. Permite desplegar demos de modelos con interfaz web (Gradio/Streamlit) gratis de forma extremadamente sencilla.
	\item \textbf{Free Tier - CPU}: 
	\begin{itemize}
		\item 2 vCPUs por Space.
		\item 16 GB de RAM.
		\item Espacio de almacenamiento: 50 GB (para modelos, datos y código).
		\item Ancho de banda: 100 MB/hour para CPUs.
		\item \textbf{Auto-off}: Los Spaces se duermen tras 48 horas de inactividad para ahorrar recursos, reactivándose con la siguiente visita.
	\end{itemize}
	\item \textbf{Free Tier - GPU (T4)}: 
	\begin{itemize}
		\item Acceso a una GPU NVIDIA T4 por Space.
		\item 16 GB de RAM.
		\item Espacio de almacenamiento: 50 GB.
		\item Ancho de banda: 30 MB/hour para GPUs.
		\item Uso: Hasta 30 horas de uso de GPU \textbf{por mes} (de forma puntual, sujeto a disponibilidad).
		\item \textbf{Auto-off}: Las GPU se apagan automáticamente tras 1 hora de inactividad.
	\end{itemize}
	\item \textbf{Enfoque}: Despliegue, demostración y sharing de modelos de IA. Integración nativa con el Hub de modelos y datasets.
	\item \textbf{URL}: \url{https://huggingface.co/spaces}
	\item \textbf{Nota}: La opción ideal para crear una demostración interactiva y pública de los resultados de tu proyecto. La gestión de la GPU es perfecta para inferencia y demostraciones, no para entrenamientos largos.
\end{itemize}

\subsubsection*{Kaggle Kernels/Notebooks}
\begin{itemize}
	\item \textbf{Tipo}: Entorno de ejecución para notebooks Jupyter en la nube (Kaggle Notebooks).
	\item \textbf{Relevancia}: Proporciona acceso gratuito a aceleradores hardware de gama alta, eliminando la barrera de entrada para entrenar modelos complejos.
	\item \textbf{Free Tier - Sesiones de Ejecución}:
	\begin{itemize}
		\item \textbf{GPU (NVIDIA Tesla P100)}: Hasta \textbf{30 horas por semana} (4.3h/día aporx.).
		\item \textbf{TPU (v3)}: Hasta \textbf{20 horas por semana} (2.8h/día aporx.).
		\item \textbf{CPU}: 20 horas por semana (sin límite de sesiones concurrentes, pero con límite de tiempo total).
	\end{itemize}
	\item \textbf{Límites por Sesión}:
	\begin{itemize}
		\item \textbf{Tiempo máximo de ejecución}: 12 horas por sesión (tras este tiempo, el kernel se detiene automáticamente).
		\item \textbf{Internet}: Los notebooks deben tener la opción de Internet activada manualmente para acceder a datos externos o instalar librerías.
		\item \textbf{Almacenamiento Volátil}: 20 GB de espacio temporal de disco (los datos no persistían entre sesiones, aunque se puede usar el sistema de datasets de Kaggle para almacenamiento persistente).
	\end{itemize}
	\item \textbf{Enfoque}: Análisis exploratorio de datos, competiciones de ML (\textit{kaggle competitions}) y, crucialmente, \textbf{entrenamiento de modelos} que requieran GPU/TPU.
	\item \textbf{URL}: \url{https://www.kaggle.com/code}
	\item \textbf{Nota}: Es la mejor plataforma gratuita para la fase de experimentación y entrenamiento de modelos de tu proyecto. Su límite semanal de GPU es muy generoso para un proyecto académico.
\end{itemize}





	\subsection{Trabajos anteriores}


\section{Inteligencia Artificial}

[TODO]

Tener en cuenta también la nueva normativa que la Unión europea ha establecido con el Reglamento de Inteligencia Artificial \citep{webRIA2024Europa}, el cual se ha tenido de base para el uso de IA en este proyecto, intentando aplicar buenas practicas al uso de las mismas, así como documentar las fuentes de datos, métodos de anonimización y posibles sesgos.
	
	\subsection{Trabajos anteriores}


